{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vme64r9PnhK6"
      },
      "source": [
        "Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOhZnP0zndVy",
        "outputId": "84c510db-241c-4f6d-b25c-4afd9158b450"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKBz9ZQqY0PP"
      },
      "source": [
        "Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ifPhdITYuMat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d82dabc-9c3d-4697-d706-f84ea38d0aa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deepchem\n",
            "  Downloading deepchem-2.7.2.dev20230504173825-py3-none-any.whl (788 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m788.2/788.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.10.1)\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2023.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.7/29.7 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.22.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2022.7.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit->deepchem) (8.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->deepchem) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->deepchem) (1.16.0)\n",
            "Installing collected packages: rdkit, deepchem\n",
            "Successfully installed deepchem-2.7.2.dev20230504173825 rdkit-2023.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting silence_tensorflow\n",
            "  Downloading silence_tensorflow-1.2.1.tar.gz (3.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting support_developer\n",
            "  Downloading support_developer-1.0.5.tar.gz (4.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: silence_tensorflow, support_developer\n",
            "  Building wheel for silence_tensorflow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for silence_tensorflow: filename=silence_tensorflow-1.2.1-py3-none-any.whl size=4475 sha256=95f5f2bbd639676088688130fdaddcd9409a9da57a3d1d734f93d9bc7ddafc78\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/2c/24/e130d6102c0df56631b9db7479d9a6a53c5d97fb06b5f61b98\n",
            "  Building wheel for support_developer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for support_developer: filename=support_developer-1.0.5-py3-none-any.whl size=5651 sha256=6806e14d33557a245bb2ac77421f56f7ad638e2b72dcee9009a60b6aeccc9140\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/72/c8/3054a5897ba0713dfa7a941364d68cbd42b0755c8e2ec1c18c\n",
            "Successfully built silence_tensorflow support_developer\n",
            "Installing collected packages: support_developer, silence_tensorflow\n",
            "Successfully installed silence_tensorflow-1.2.1 support_developer-1.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install --pre deepchem\n",
        "!pip install silence_tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Statements"
      ],
      "metadata": {
        "id": "yrWkmdrdptnQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Mi4uk7Y5Yz2M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "858a6e26-d776-48cb-9650-f43c1e47177c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:deepchem.feat.molecule_featurizers.rdkit_descriptors:No normalization for AvgIpc. Feature removed!\n",
            "WARNING:deepchem.models.torch_models:Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
            "WARNING:deepchem.models.torch_models:Skipped loading modules with transformers dependency. No module named 'transformers'\n",
            "WARNING:deepchem.models:cannot import name 'HuggingFaceModel' from 'deepchem.models.torch_models' (/usr/local/lib/python3.10/dist-packages/deepchem/models/torch_models/__init__.py)\n",
            "WARNING:deepchem.models:Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/usr/local/lib/python3.10/dist-packages/deepchem/models/torch_models/__init__.py)\n",
            "WARNING:deepchem.models:Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'pytorch_lightning'\n",
            "WARNING:deepchem.models:Skipped loading some Jax models, missing a dependency. No module named 'haiku'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import io\n",
        "import pickle\n",
        "import shutil\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import seaborn as sns\n",
        "from random import seed\n",
        "from random import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import combinations\n",
        "\n",
        "import deepchem as dc\n",
        "from deepchem.feat.mol_graphs import ConvMol, WeaveMol\n",
        "from deepchem.models.layers import GraphConv, GraphPool, GraphGather\n",
        "\n",
        "from keras import backend as K \n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Reshape, LSTM, Dropout, BatchNormalization, Conv2D, Flatten\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "from scipy.stats import wasserstein_distance\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as layers\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "from rdkit.Chem import rdMolDescriptors\n",
        "from rdkit.Chem.Draw import SimilarityMaps\n",
        "\n",
        "#Silence Tensorflow warnings\n",
        "from silence_tensorflow import silence_tensorflow\n",
        "silence_tensorflow()\n",
        "\n",
        "#Seed Randomness\n",
        "seed(13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrUy83GQZqEA"
      },
      "source": [
        "Load Dataset into 5 seperate folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MtEXfPgNZpee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c55adcfa-144c-4446-9e2d-759fa8d4ded1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1501\n",
            "1503\n",
            "1500\n",
            "1505\n",
            "1496\n"
          ]
        }
      ],
      "source": [
        "file_to_read = open(\"/content/drive/MyDrive/Colab Notebooks/NIST Dataset.pickle\", \"rb\")\n",
        "\n",
        "d = pickle.load(file_to_read)\n",
        "smiles = d[\"smiles\"]\n",
        "sequences = d[\"sequences\"]\n",
        "that_index = int(np.where(smiles == \"C\")[0]) #This single carbon node (methane) doesn't work for some of the graphs, so we manually exclude it \n",
        "smiles = np.concatenate((smiles[:that_index], smiles[that_index+1:]))\n",
        "sequences = np.concatenate((sequences[:that_index], sequences[that_index+1:]))\n",
        "\n",
        "#Zip each data sequence\n",
        "dataset = list(zip(smiles, sequences))\n",
        "shuffle(dataset)\n",
        "\n",
        "#Extract compounds that occur more than once so that repeats aren't distributed across folds\n",
        "single_occurence_molecules = [x for x in dataset if list(d[\"smiles\"]).count(x[0]) <= 1]\n",
        "multiple_occurence_molecules = [x for x in dataset if x[0] not in [h[0] for h in single_occurence_molecules]]\n",
        "multis = multiple_occurence_molecules\n",
        "\n",
        "#Create folds\n",
        "folds = {}\n",
        "fold_size = len(single_occurence_molecules) // 5\n",
        "for i in range(1, 6):\n",
        "    folds[i] = single_occurence_molecules[((i - 1) * fold_size):(i * fold_size)]\n",
        "\n",
        "#Add whatever wasn't added from signgle occurences to the end of multiple occurences\n",
        "multiple_occurence_molecules += single_occurence_molecules[(5 * fold_size):]\n",
        "mult_fold_size = len(multiple_occurence_molecules) // 5\n",
        "\n",
        "#Add all these molecules across folds such that all repeat occurences always occur within the same fold\n",
        "current_fold = 0\n",
        "while(len(multiple_occurence_molecules) > 0):\n",
        "    current_fold %= 5\n",
        "    current_fold += 1\n",
        "    current_molecule = multiple_occurence_molecules[0]\n",
        "    while current_molecule[0] in [h[0] for h in multiple_occurence_molecules]:\n",
        "        folds[current_fold].append(multiple_occurence_molecules.pop([h[0] for h in multiple_occurence_molecules].index(current_molecule[0])))\n",
        "\n",
        "#Print the length of each fold \n",
        "for i in range(1, 6):\n",
        "    print(len(folds[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0DdmocNqkwJ"
      },
      "source": [
        "Create test and train sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "INb1sVrcqiAV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06f5aafd-ad7d-419f-a53c-e7b8503214e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-39d36c2dcfca>:5: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  scale = 1 / maxval\n"
          ]
        }
      ],
      "source": [
        "#Helper Functions\n",
        "def normalize(s):\n",
        "    \"\"\"Normalize the input series from 0->1 and return it\"\"\"\n",
        "    maxval = max(s)\n",
        "    scale = 1 / maxval\n",
        "    if(maxval == 0):\n",
        "      scale = 0\n",
        "    return([j * scale for j in s])\n",
        "\n",
        "def floor_out(x):\n",
        "    \"\"\"Add a floor threshold of 0.01 to reduce noise in spectra\"\"\"\n",
        "    return([j if j > 0.01 else 0 for j in x])\n",
        "\n",
        "def normal_many(x):\n",
        "    \"\"\"Normalize and floor in series\"\"\"\n",
        "    return(np.array([floor_out(normalize(j)) for j in x]))\n",
        "\n",
        "#Create fold sets\n",
        "dataset_splits = {1: {}, 2: {}, 3: {}, 4: {}, 5: {}}\n",
        "for i in range(1, 6):\n",
        "  #For each i-th split, the testing set will be the i-th fold\n",
        "  test = folds[i]\n",
        "  train = []\n",
        "  for x in range(1, 6):\n",
        "    if x != i:\n",
        "      train += folds[x]\n",
        "  \n",
        "  dataset_splits[i][\"test_smiles\"] = [j[0] for j in test]\n",
        "  dataset_splits[i][\"test_y\"] = normal_many([j[1] for j in test])[:,432:] \n",
        "  dataset_splits[i][\"train_smiles\"] = [j[0] for j in train]\n",
        "  dataset_splits[i][\"train_y\"] = normal_many([j[1] for j in train])[:,432:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN6YNridFKyx"
      },
      "source": [
        "Define Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wrwfhVyP8s-i"
      },
      "outputs": [],
      "source": [
        "def euc_dist_keras(y_true, y_pred):\n",
        "    \"\"\"Euclidean distance loss function\"\"\"\n",
        "    return K.sqrt(K.sum(K.square(y_true - y_pred), axis=-1, keepdims=True))\n",
        "\n",
        "def pearson_first(y_true, y_pred):\n",
        "    \"\"\"Return pearson correlation for two single tensors\"\"\"\n",
        "    return(pearsonr(y_true, y_pred)[0])\n",
        "\n",
        "def wrapped_pearson_correlation(y_true, y_pred):\n",
        "    y = tf.py_function(func = pearson_first, inp = [y_true, y_pred], Tout = tf.float32)\n",
        "    return(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS4054Nmvl-N"
      },
      "source": [
        "Run SMILES through DC Featurizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "REoPd3un1a3g"
      },
      "outputs": [],
      "source": [
        "featurizer = dc.feat.CircularFingerprint(radius = 2, size = 1024, chiral = False, features = False)\n",
        "for i in range(1, 6):\n",
        "  dataset_splits[i][\"test_x\"] = featurizer.featurize(dataset_splits[i][\"test_smiles\"])\n",
        "  dataset_splits[i][\"train_x\"] = featurizer.featurize(dataset_splits[i][\"train_smiles\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP5Oz1pzuriK"
      },
      "source": [
        "Add Graph Featurization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uhyi3MLFup8R"
      },
      "outputs": [],
      "source": [
        "graph_featurizer = dc.feat.ConvMolFeaturizer()\n",
        "for i in range(1, 6):\n",
        "  dataset_splits[i][\"test_x_graph\"] = graph_featurizer.featurize(dataset_splits[i][\"test_smiles\"])\n",
        "  dataset_splits[i][\"train_x_graph\"] = graph_featurizer.featurize(dataset_splits[i][\"train_smiles\"])\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCpPQlVEvkh5"
      },
      "source": [
        "Graph Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JfuUHQyvj8d"
      },
      "outputs": [],
      "source": [
        "def data_generator(dataset, epochs = 1):\n",
        "    for ind, (X_b, y_b, w_b, ids_b) in enumerate(dataset.iterbatches(batch_size, epochs, deterministic = True, pad_batches = True)):\n",
        "        multiConvMol = ConvMol.agglomerate_mols(X_b)\n",
        "        inputs = [multiConvMol.get_atom_features(), multiConvMol.deg_slice, multiConvMol.membership]\n",
        "        for i in range(1, len(multiConvMol.get_deg_adjacency_lists())):\n",
        "            inputs.append(multiConvMol.get_deg_adjacency_lists()[i])\n",
        "            inputs = np.array(inputs)\n",
        "            labels = [y_b]\n",
        "            weights = [w_b]\n",
        "\n",
        "    yield (inputs, labels, weights)\n",
        "\n",
        "def clean(arr):\n",
        "    #Helper Function for DC featurizers\n",
        "    arr = list(map(float, arr))\n",
        "    return [item for item in arr if not np.isnan(item)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGbbJTxuYf0H"
      },
      "source": [
        "Train Each Graph Fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQq8KIl87FzZ"
      },
      "source": [
        "Dense Models Featurizers and Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEeHjtRb7JrL"
      },
      "outputs": [],
      "source": [
        "#GraphConv featurization\n",
        "graph_featurizer = dc.feat.ConvMolFeaturizer()\n",
        "for i in range(1, 6):\n",
        "  dataset_splits[i][\"test_x_graph\"] = graph_featurizer.featurize(dataset_splits[i][\"test_smiles\"])\n",
        "  dataset_splits[i][\"train_x_graph\"] = graph_featurizer.featurize(dataset_splits[i][\"train_smiles\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YO6EuPF17NIS"
      },
      "outputs": [],
      "source": [
        "#Weave featurization for MPNN\n",
        "mpnn = dc.feat.WeaveFeaturizer()\n",
        "for i in range(1, 6):\n",
        "  dataset_splits[i][\"test_x_mpnn\"] = mpnn.featurize(dataset_splits[i][\"test_smiles\"])\n",
        "  dataset_splits[i][\"train_x_mpnn\"] = mpnn.featurize(dataset_splits[i][\"train_smiles\"])\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MPNN"
      ],
      "metadata": {
        "id": "lY9YslEL_xX_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qe9aLN6y9WJs",
        "outputId": "6db6479b-6158-4e4d-8955-dcb85fd2a6d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using backend: pytorch\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2 Loss for fold 1 : 0.87099\n",
            "I: 1 Mean 0.8709891633702062 Median 0.9148299098014832 STDev 0.14196605216820152\n",
            "R2 Loss for fold 2 : 0.86357\n",
            "I: 2 Mean 0.8635666554884184 Median 0.9149425625801086 STDev 0.1539242824899847\n",
            "R2 Loss for fold 3 : 0.87453\n",
            "I: 3 Mean 0.8745304910739263 Median 0.9209468364715576 STDev 0.13286753394083609\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2 Loss for fold 4 : 0.86081\n",
            "I: 4 Mean 0.8613861383810798 Median 0.9138676822185516 STDev 0.1590294293532036\n",
            "R2 Loss for fold 5 : 0.87333\n",
            "I: 5 Mean 0.873326017391833 Median 0.9191482663154602 STDev 0.14286206732989026\n"
          ]
        }
      ],
      "source": [
        "#MPNN model training loop\n",
        "\n",
        "from deepchem.models.torch_models import MPNNModel\n",
        "\"\"\"\n",
        "dtrain = dc.data.NumpyDataset(X = dataset_splits[i][\"train_x_mpnn\"], y = dataset_splits[i][\"train_y\"])\n",
        "dtest = dc.data.NumpyDataset(X = dataset_splits[i][\"test_x_mpnn\"], y = dataset_splits[i][\"test_y\"])\"\"\"\n",
        "for i in range(1, 6):\n",
        "    dtrain = dc.data.NumpyDataset(X = dataset_splits[i][\"train_x_mgc\"], y = dataset_splits[i][\"train_y\"])\n",
        "    dtest = dc.data.NumpyDataset(X = dataset_splits[i][\"test_x_mgc\"], y = dataset_splits[i][\"test_y\"])\n",
        "    model = MPNNModel(1800, mode='regression', dropout = 0.1, batch_normalize = True, dense_layer_size=2048, batch_size = 64, n_pair_feat = 14, n_atom_feat = 75)\n",
        "    model.fit(dtrain, nb_epoch = 100)\n",
        "    #Collect evaluation metrics\n",
        "    graph_r2s = []\n",
        "    g2predictions = model.predict(dtest)\n",
        "    total_r2, count = 0, 0\n",
        "    for x in range(len(dataset_splits[i][\"test_y\"])):\n",
        "        current_r2 = wrapped_pearson_correlation(normalize(g2predictions[x]), dataset_splits[i][\"test_y\"][x])\n",
        "        total_r2 += 0 if np.isnan(current_r2) else current_r2 \n",
        "        graph_r2s.append(current_r2)\n",
        "        count += 1\n",
        "    current_fold_loss = round(float(total_r2 / count), 5)\n",
        "    print(\"R2 Loss for fold\", i, \":\", current_fold_loss)\n",
        "    gr2 = clean(list(map(float, graph_r2s)))\n",
        "    print(\"I:\", i, \"Mean\", statistics.mean(gr2), \"Median\", statistics.median(gr2), \"STDev\", statistics.stdev(gr2))\n",
        "    fold_predictions_path = path + \"MPNN_\" + str(i) + \"_preds.pickle\"\n",
        "    with open(fold_predictions_path, 'wb') as handle:\n",
        "        pickle.dump(g2predictions, handle)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AttentiveFP "
      ],
      "metadata": {
        "id": "HVI2Y6f__u0b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUnOQjXJ9cjH",
        "outputId": "8fb61b15-2bbe-48d1-ef63-b67e4b715ec8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2 Loss for fold 1 : 0.89004\n",
            "I: 1 Mean 0.8900437510084066 Median 0.9359551072120667 STDev 0.13420706069836466\n",
            "R2 Loss for fold 2 : 0.88568\n",
            "I: 2 Mean 0.8856813292343142 Median 0.9355883002281189 STDev 0.13951395890818105\n",
            "R2 Loss for fold 3 : 0.89097\n",
            "I: 3 Mean 0.8909696869750817 Median 0.9375252425670624 STDev 0.12860900264472283\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2 Loss for fold 4 : 0.88039\n",
            "I: 4 Mean 0.8809766138871596 Median 0.9358482956886292 STDev 0.1528139015404319\n",
            "R2 Loss for fold 5 : 0.88751\n",
            "I: 5 Mean 0.8875074465734197 Median 0.9358925521373749 STDev 0.14031811902949104\n"
          ]
        }
      ],
      "source": [
        "#AttentiveFP model training loop\n",
        "import deepchem as dc\n",
        "from deepchem.models import AttentiveFPModel\n",
        "for i in range(1, 6):\n",
        "    dtrain = dc.data.NumpyDataset(X = dataset_splits[i][\"train_x_mgc\"], y = dataset_splits[i][\"train_y\"])\n",
        "    dtest = dc.data.NumpyDataset(X = dataset_splits[i][\"test_x_mgc\"], y = dataset_splits[i][\"test_y\"])\n",
        "    fpmodel = AttentiveFPModel(n_tasks = 1800, mode='regression', dropout = 0.1, batch_normalize = True, dense_layer_size=2048, batch_size = 64, learning_rate = 0.001, optimizer = \"Adam\", activation_fns = \"p\")\n",
        "\n",
        "    fpmodel.fit(dtrain, nb_epoch = 100)\n",
        "    fppredictions = fpmodel.predict(dtest)\n",
        "\n",
        "    graph_r2s = []\n",
        "    total_r2, count = 0, 0\n",
        "    for x in range(len(dataset_splits[i][\"test_y\"])):\n",
        "        current_r2 = wrapped_pearson_correlation(normalize(fppredictions[x]), dataset_splits[i][\"test_y\"][x])\n",
        "        total_r2 += 0 if np.isnan(current_r2) else current_r2 \n",
        "        graph_r2s.append(current_r2)\n",
        "        count += 1\n",
        "    current_fold_loss = round(float(total_r2 / count), 5)\n",
        "    print(\"R2 Loss for fold\", i, \":\", current_fold_loss)\n",
        "    gr2 = clean(list(map(float, graph_r2s)))\n",
        "    print(\"I:\", i, \"Mean\", statistics.mean(gr2), \"Median\", statistics.median(gr2), \"STDev\", statistics.stdev(gr2))#expirement\n",
        "    fold_predictions_path = path + \"AFP_\" + str(i) + \"_preds.pickle\"\n",
        "    with open(fold_predictions_path, 'wb') as handle:\n",
        "        pickle.dump(fppredictions, handle)    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GAT Model"
      ],
      "metadata": {
        "id": "lIOsF3_t_sBg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9xEjxDR9fH_",
        "outputId": "aa499240-0e18-43ba-ef3e-07bff6ef6784"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
            "R2 Loss for fold 1 : 0.80144\n",
            "I: 1 Mean 0.8014362164334686 Median 0.8348127603530884 STDev 0.15754606698103107\n",
            "R2 Loss for fold 2 : 0.80858\n",
            "I: 2 Mean 0.8085786684361086 Median 0.8482959270477295 STDev 0.15179206103721388\n",
            "R2 Loss for fold 3 : 0.80534\n",
            "I: 3 Mean 0.8053351333787044 Median 0.8364363312721252 STDev 0.14961929769952964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 Loss for fold 4 : 0.80272\n",
            "I: 4 Mean 0.8032512624049559 Median 0.8420479595661163 STDev 0.16012092895721208\n",
            "R2 Loss for fold 5 : 0.80157\n",
            "I: 5 Mean 0.801570797378302 Median 0.831894725561142 STDev 0.1479842578919792\n"
          ]
        }
      ],
      "source": [
        "#GAT model training loop\n",
        "for i in range(1, 6):\n",
        "    dtrain = dc.data.NumpyDataset(X = dataset_splits[i][\"train_x_mgc\"], y = dataset_splits[i][\"train_y\"])\n",
        "    dtest = dc.data.NumpyDataset(X = dataset_splits[i][\"test_x_mgc\"], y = dataset_splits[i][\"test_y\"])\n",
        "    #64, 64 imrpobed it\n",
        "    model = dc.models.GATModel(1800, mode='regression', dropout = 0.1, graph_attention_layers = [64, 64], batch_normalize = True, dense_layer_size=2048, batch_size = 64, learning_rate = 0.001)\n",
        "    model.fit(dtrain, nb_epoch = 100)\n",
        "    gatpredictions = model.predict(dtest)\n",
        "\n",
        "    graph_r2s = []\n",
        "    total_r2, count = 0, 0\n",
        "    for x in range(len(dataset_splits[i][\"test_y\"])):\n",
        "        current_r2 = wrapped_pearson_correlation(normalize(gatpredictions[x]), dataset_splits[i][\"test_y\"][x])\n",
        "        total_r2 += 0 if np.isnan(current_r2) else current_r2 \n",
        "        graph_r2s.append(current_r2)\n",
        "        count += 1\n",
        "    current_fold_loss = round(float(total_r2 / count), 5)\n",
        "    print(\"R2 Loss for fold\", i, \":\", current_fold_loss)\n",
        "    gr2 = clean(list(map(float, graph_r2s)))\n",
        "    print(\"I:\", i, \"Mean\", statistics.mean(gr2), \"Median\", statistics.median(gr2), \"STDev\", statistics.stdev(gr2))#expirement\n",
        "    \n",
        "    fold_predictions_path = path + \"GAT_\" + str(i) + \"_preds.pickle\"\n",
        "    with open(fold_predictions_path, 'wb') as handle:\n",
        "        pickle.dump(gatpredictions, handle) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MorganFP/DNN Model"
      ],
      "metadata": {
        "id": "aecHV3H6_pgl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iId76mTgKd3c",
        "outputId": "06f7ba78-3415-4c4e-bc50-21fd12d2dfd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2 Loss for fold 1 : 0.86765\n",
            "I 1 0.8676516930898435 0.9199491143226624 0.15140122721398225\n",
            "R2 Loss for fold 2 : 0.8645\n",
            "I 2 0.8644993320399931 0.9224254488945007 0.1577399099835786\n",
            "R2 Loss for fold 3 : 0.86978\n",
            "I 3 0.8697819181475789 0.92546346783638 0.147056175220565\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2 Loss for fold 4 : 0.85942\n",
            "I 4 0.8599970634491678 0.9204980432987213 0.16062992959480313\n",
            "R2 Loss for fold 5 : 0.86863\n",
            "I 5 0.8686325302681224 0.9234545826911926 0.1527444680999287\n"
          ]
        }
      ],
      "source": [
        "#MorganFP model using a dense layer as output\n",
        "for i in range(1, 6):\n",
        "    fpmodel = Sequential()\n",
        "    fpmodel.add(Dense(4096, input_dim = 1024))\n",
        "    fpmodel.add(BatchNormalization())\n",
        "    fpmodel.add(Dropout(0.1))\n",
        "    fpmodel.add(Dense(2048, activation = \"relu\"))\n",
        "    fpmodel.add(BatchNormalization())\n",
        "    fpmodel.add(Dropout(0.1))\n",
        "    fpmodel.add(Dense(1024, activation = \"relu\"))\n",
        "\n",
        "    fpmodel.add(Dense(1800, activation = \"sigmoid\"))\n",
        "\n",
        "    #opt = Adam(learning_rate = 0.001)\n",
        "\n",
        "    fpmodel.compile(loss = euc_dist_keras, optimizer = \"Adam\")\n",
        "    fpmodel.fit(dataset_splits[i][\"train_x\"], dataset_splits[i][\"train_y\"], batch_size = 64, epochs = 100, verbose = 0)\n",
        "    #Collect evaluation metrics\n",
        "    morganpredictions = fpmodel.predict(dataset_splits[i][\"test_x\"])\n",
        "    total_r2, count = 0, 0\n",
        "    totalp = 0\n",
        "    fp_r2s = []\n",
        "    for x in range(len(morganpredictions)):\n",
        "        current_r2 = wrapped_pearson_correlation(normalize(morganpredictions[x]), dataset_splits[i][\"test_y\"][x])\n",
        "        total_r2 += 0 if np.isnan(current_r2) else current_r2 \n",
        "        fp_r2s.append(current_r2)\n",
        "        count += 1\n",
        "\n",
        "    current_fold_loss = round(float(total_r2 / count), 5)\n",
        "    print(\"R2 Loss for fold\", i, \":\", current_fold_loss)\n",
        "    cleanfpr2s = clean(list(map(float, fp_r2s)))\n",
        "    print(\"I\", i, statistics.mean(cleanfpr2s), statistics.median(cleanfpr2s), statistics.stdev(cleanfpr2s))\n",
        "    fold_predictions_path = path + \"MFP_\" + str(i) + \"_preds.pickle\"\n",
        "    with open(fold_predictions_path, 'wb') as handle:\n",
        "        pickle.dump(morganpredictions, handle)    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rl3Xjsl4ObNP"
      },
      "source": [
        "Load up pickle files of saved predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTdUKF6EOZ4j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55f86971-1249-4fcb-e129-60dde85d051f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7505"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "path = \"/content/drive/MyDrive/Miscellaneous Code/SpectraPredictions/\"\n",
        "all_predictions = {'GC': [], 'MPNN': [], 'AFP': [], 'GAT': [], 'MFP': []}\n",
        "for model_string in ('GC_', 'MPNN_', 'AFP_', 'GAT_', 'MFP_'):\n",
        "    for i in range(1, 6):\n",
        "        fold_predictions_path = path + model_string + str(i) + \"_preds.pickle\"\n",
        "        with open(fold_predictions_path, 'rb') as handle:\n",
        "            all_predictions[model_string[:-1]].extend(pickle.load(handle))\n",
        "len(all_predictions['GC'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDK7nMl_SEhh",
        "outputId": "717a0f6f-8d58-4166-8e82-d7dec7e97429"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-c52eb143fa4e>:75: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  scale = 1 / maxval\n",
            "/usr/local/lib/python3.8/dist-packages/scipy/stats/stats.py:4023: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GC 0.79164\n",
            "MPNN 0.83014\n",
            "AFP 0.84906\n",
            "GAT 0.83778\n",
            "MFP 0.84342\n"
          ]
        }
      ],
      "source": [
        "#Normal average manually added up from model predictions\n",
        "total_r2, count = 0, 0\n",
        "all_r2 = {'GC': [], 'MPNN': [], 'AFP': [], 'GAT': [], 'MFP': []}\n",
        "for model_string in ('GC', 'MPNN', 'AFP', 'GAT', 'MFP'):\n",
        "    for x in range(7505):\n",
        "        current_r2 = wrapped_pearson_correlation(normalize(all_predictions[model_string][x]), normalize(all_labels[x]))\n",
        "        all_r2[model_string].append(current_r2)\n",
        "        total_r2 += 0 if np.isnan(current_r2) else current_r2 \n",
        "        count += 1\n",
        "    print(model_string, round(float(total_r2 / count), 5)) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This block does both flooring and gaus smoothing at the right value \n",
        "def floor_pred(pred):\n",
        "    floored_pred = [absorbance if absorbance >= 0 else 0 for absorbance in pred]\n",
        "    return floored_pred\n",
        "import scipy\n",
        "total_r2, count = 0, 0\n",
        "all_r2_flat_smooth = {'GC': [], 'MPNN': [], 'AFP': [], 'GAT': [], 'MFP': []}\n",
        "#all_r2_flat_smooth = {'AFP': []}\n",
        "for model_string in all_r2_flat_smooth.keys():\n",
        "    for x in range(7505):\n",
        "        current_r2 = wrapped_pearson_correlation(scipy.ndimage.gaussian_filter1d(floor_pred(normalize(all_predictions[model_string][x])), 3), normalize(all_labels[x]))\n",
        "        all_r2_flat_smooth[model_string].append(current_r2)\n",
        "        total_r2 += 0 if np.isnan(current_r2) else current_r2 \n",
        "        count += 1\n",
        "    print(model_string, round(float(total_r2 / count), 5)) "
      ],
      "metadata": {
        "id": "U5PQrJUe2e8j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "927d6f37-6779-479a-a549-2408c4fe231b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-c52eb143fa4e>:75: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  scale = 1 / maxval\n",
            "/usr/local/lib/python3.8/dist-packages/scipy/stats/stats.py:4023: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GC 0.80137\n",
            "MPNN 0.83668\n",
            "AFP 0.85424\n",
            "GAT 0.84324\n",
            "MFP 0.84791\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}