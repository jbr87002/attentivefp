{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vme64r9PnhK6"
      },
      "source": [
        "Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOhZnP0zndVy",
        "outputId": "9ea1442b-7be0-4bc2-92e0-4b48ceb11b6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8VN9QvocIfN",
        "outputId": "f2354dff-f8d4-4973-c520-bf2ce3f1c278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKBz9ZQqY0PP"
      },
      "source": [
        "Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifPhdITYuMat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afec9ae5-ee56-449a-af11-0bd327e5cbb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.dgl.ai/wheels/cu118/repo.html\n",
            "Collecting dgl\n",
            "  Downloading https://data.dgl.ai/wheels/cu118/dgl-1.1.0%2Bcu118-cp310-cp310-manylinux1_x86_64.whl (86.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.4/86.4 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.10.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.65.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.4)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-1.1.0+cu118\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.dgl.ai/wheels-test/repo.html\n",
            "Collecting dglgo\n",
            "  Downloading dglgo-0.0.2-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dglgo) (0.7.0)\n",
            "Collecting isort>=5.10.1 (from dglgo)\n",
            "  Downloading isort-5.12.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autopep8>=1.6.0 (from dglgo)\n",
            "  Downloading autopep8-2.0.2-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpydoc>=1.1.0 (from dglgo)\n",
            "  Downloading numpydoc-1.5.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.4/52.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from dglgo) (1.10.7)\n",
            "Collecting ruamel.yaml>=0.17.20 (from dglgo)\n",
            "  Downloading ruamel.yaml-0.17.31-py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.1/112.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from dglgo) (6.0)\n",
            "Collecting ogb>=1.3.3 (from dglgo)\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rdkit-pypi (from dglgo)\n",
            "  Downloading rdkit_pypi-2022.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from dglgo) (1.2.2)\n",
            "Collecting pycodestyle>=2.10.0 (from autopep8>=1.6.0->dglgo)\n",
            "  Downloading pycodestyle-2.10.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from autopep8>=1.6.0->dglgo) (2.0.1)\n",
            "Collecting sphinx>=4.2 (from numpydoc>=1.1.0->dglgo)\n",
            "  Downloading sphinx-7.0.1-py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.10/dist-packages (from numpydoc>=1.1.0->dglgo) (3.1.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (4.65.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.26.15)\n",
            "Collecting outdated>=0.2.0 (from ogb>=1.3.3->dglgo)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.0->dglgo) (4.5.0)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.20->dglgo)\n",
            "  Downloading ruamel.yaml.clib-0.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (485 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->dglgo) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->dglgo) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->dglgo) (3.1.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.4.0->dglgo) (8.1.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi->dglgo) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.10->numpydoc>=1.1.0->dglgo) (2.1.2)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb>=1.3.3->dglgo) (67.7.2)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb>=1.3.3->dglgo)\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb>=1.3.3->dglgo) (2.27.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2022.7.1)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.4)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.2)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.1.5)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.3)\n",
            "Requirement already satisfied: Pygments>=2.13 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.14.0)\n",
            "Collecting docutils<0.21,>=0.18.1 (from sphinx>=4.2->numpydoc>=1.1.0->dglgo)\n",
            "  Downloading docutils-0.20.1-py3-none-any.whl (572 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.7/572.7 kB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: snowballstemmer>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.9 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.12.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (0.7.13)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.4.1)\n",
            "Requirement already satisfied: packaging>=21.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb>=1.3.3->dglgo) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb>=1.3.3->dglgo) (16.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb>=1.3.3->dglgo) (1.3.0)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7029 sha256=a1cad476776f7b47b215124630526af01b750b609526a17951f11d9695b352e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, ruamel.yaml.clib, rdkit-pypi, pycodestyle, isort, docutils, sphinx, ruamel.yaml, outdated, autopep8, numpydoc, ogb, dglgo\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.16\n",
            "    Uninstalling docutils-0.16:\n",
            "      Successfully uninstalled docutils-0.16\n",
            "  Attempting uninstall: sphinx\n",
            "    Found existing installation: Sphinx 3.5.4\n",
            "    Uninstalling Sphinx-3.5.4:\n",
            "      Successfully uninstalled Sphinx-3.5.4\n",
            "Successfully installed autopep8-2.0.2 dglgo-0.0.2 docutils-0.20.1 isort-5.12.0 littleutils-0.2.2 numpydoc-1.5.0 ogb-1.3.6 outdated-0.2.2 pycodestyle-2.10.0 rdkit-pypi-2022.9.5 ruamel.yaml-0.17.31 ruamel.yaml.clib-0.2.7 sphinx-7.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dgllife\n",
            "  Downloading dgllife-0.3.2-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.1/226.1 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.10/dist-packages (from dgllife) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from dgllife) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from dgllife) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgllife) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgllife) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgllife) (1.10.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgllife) (3.1)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.10/dist-packages (from dgllife) (0.2.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from dgllife) (1.2.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->dgllife) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->dgllife) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->dgllife) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->dgllife) (3.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->dgllife) (3.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from hyperopt->dgllife) (1.16.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt->dgllife) (0.18.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt->dgllife) (2.2.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt->dgllife) (0.10.9.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->dgllife) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->dgllife) (2022.7.1)\n",
            "Installing collected packages: dgllife\n",
            "Successfully installed dgllife-0.3.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deepchem\n",
            "  Downloading deepchem-2.7.1-py3-none-any.whl (693 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m693.2/693.2 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.2.2)\n",
            "Collecting scipy<1.9 (from deepchem)\n",
            "  Downloading scipy-1.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rdkit (from deepchem)\n",
            "  Downloading rdkit-2023.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.7/29.7 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2022.7.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit->deepchem) (8.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->deepchem) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->deepchem) (1.16.0)\n",
            "Installing collected packages: scipy, rdkit, deepchem\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.10.1\n",
            "    Uninstalling scipy-1.10.1:\n",
            "      Successfully uninstalled scipy-1.10.1\n",
            "Successfully installed deepchem-2.7.1 rdkit-2023.3.1 scipy-1.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip install  dgl -f https://data.dgl.ai/wheels/cu118/repo.html\n",
        "!pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html\n",
        "!pip install dgllife\n",
        "!pip install torch\n",
        "!pip install deepchem"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Statements"
      ],
      "metadata": {
        "id": "yrWkmdrdptnQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mi4uk7Y5Yz2M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6da25e3f-5eb1-4c00-a349-f158affc5fd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:deepchem.models.torch_models:Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
            "WARNING:deepchem.models:Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/usr/local/lib/python3.10/dist-packages/deepchem/models/torch_models/__init__.py)\n",
            "WARNING:deepchem.models:Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'pytorch_lightning'\n",
            "WARNING:deepchem.models:Skipped loading some Jax models, missing a dependency. No module named 'haiku'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import io\n",
        "import pickle\n",
        "import shutil\n",
        "import warnings\n",
        "import statistics\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import seaborn as sns\n",
        "from random import seed\n",
        "from random import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import combinations\n",
        "\n",
        "import deepchem as dc\n",
        "from deepchem.feat.mol_graphs import ConvMol, WeaveMol\n",
        "from deepchem.models.layers import GraphConv, GraphPool, GraphGather\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Reshape, LSTM, Dropout, BatchNormalization, Conv2D, Flatten\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "from scipy.stats import wasserstein_distance\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as layers\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "from rdkit.Chem import rdMolDescriptors\n",
        "from rdkit.Chem.Draw import SimilarityMaps\n",
        "\n",
        "#Seed\n",
        "seed(13)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Path to load/save models and predictions"
      ],
      "metadata": {
        "id": "KpTJJMMTeLZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/Miscellaneous Code/SpectraPredictions/\""
      ],
      "metadata": {
        "id": "mQ-5IhJIeJsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrUy83GQZqEA"
      },
      "source": [
        "Load Dataset into 5 seperate folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtEXfPgNZpee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3a1f994-daf5-4852-d8cc-34cd1075e5bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1501\n",
            "1503\n",
            "1500\n",
            "1505\n",
            "1496\n"
          ]
        }
      ],
      "source": [
        "file_to_read = open(\"/content/drive/MyDrive/Colab Notebooks/NIST Dataset.pickle\", \"rb\")\n",
        "\n",
        "d = pickle.load(file_to_read)\n",
        "smiles = d[\"smiles\"]\n",
        "sequences = d[\"sequences\"]\n",
        "that_index = int(np.where(smiles == \"C\")[0]) #This single carbon node (methane) breaks the MolGraphConv featurizer below, so this compound is manualy removed\n",
        "smiles = np.concatenate((smiles[:that_index], smiles[that_index+1:]))\n",
        "sequences = np.concatenate((sequences[:that_index], sequences[that_index+1:]))\n",
        "\n",
        "#Zip each data sequence\n",
        "dataset = list(zip(smiles, sequences))\n",
        "shuffle(dataset)\n",
        "\n",
        "#Extract compounds that occur more than once so that repeats aren't distributed across folds\n",
        "single_occurence_molecules = [x for x in dataset if list(d[\"smiles\"]).count(x[0]) <= 1]\n",
        "multiple_occurence_molecules = [x for x in dataset if x[0] not in [h[0] for h in single_occurence_molecules]]\n",
        "multis = multiple_occurence_molecules\n",
        "\n",
        "#Create folds\n",
        "folds = {}\n",
        "fold_size = len(single_occurence_molecules) // 5\n",
        "for i in range(1, 6):\n",
        "    folds[i] = single_occurence_molecules[((i - 1) * fold_size):(i * fold_size)]\n",
        "\n",
        "#Add whatever wasn't added from single occurences to the end of multiple occurences\n",
        "multiple_occurence_molecules += single_occurence_molecules[(5 * fold_size):]\n",
        "mult_fold_size = len(multiple_occurence_molecules) // 5\n",
        "\n",
        "#Add all these molecules across folds such that all repeat occurences always occur within the same fold\n",
        "current_fold = 0\n",
        "while(len(multiple_occurence_molecules) > 0):\n",
        "    current_fold %= 5\n",
        "    current_fold += 1\n",
        "    current_molecule = multiple_occurence_molecules[0]\n",
        "    while current_molecule[0] in [h[0] for h in multiple_occurence_molecules]:\n",
        "        folds[current_fold].append(multiple_occurence_molecules.pop([h[0] for h in multiple_occurence_molecules].index(current_molecule[0])))\n",
        "\n",
        "#Print the length of each fold\n",
        "for i in range(1, 6):\n",
        "    print(len(folds[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0DdmocNqkwJ"
      },
      "source": [
        "Create test and train sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INb1sVrcqiAV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba79ce14-ef44-46f3-f189-994f7a783e8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-f48fcfdff6d8>:5: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  scale = 1 / maxval\n"
          ]
        }
      ],
      "source": [
        "#Helper Functions\n",
        "def normalize(s):\n",
        "    \"\"\"Normalize the input series from 0->1 and return it\"\"\"\n",
        "    maxval = max(s)\n",
        "    scale = 1 / maxval\n",
        "    if(maxval == 0):\n",
        "      scale = 0\n",
        "    return([j * scale for j in s])\n",
        "\n",
        "def floor_out(x):\n",
        "    \"\"\"Add a floor threshold of 0.01 to reduce noise in spectra\"\"\"\n",
        "    return([j if j > 0.01 else 0 for j in x])\n",
        "\n",
        "def normal_many(x):\n",
        "    \"\"\"Normalize and floor in series\"\"\"\n",
        "    return(np.array([floor_out(normalize(j)) for j in x]))\n",
        "\n",
        "#Create fold sets\n",
        "dataset_splits = {1: {}, 2: {}, 3: {}, 4: {}, 5: {}}\n",
        "for i in range(1, 6):\n",
        "  #For each i-th split, the testing set will be the i-th fold\n",
        "  test = folds[i]\n",
        "  train = []\n",
        "  for x in range(1, 6):\n",
        "    if x != i:\n",
        "      train += folds[x]\n",
        "\n",
        "  dataset_splits[i][\"test_smiles\"] = [j[0] for j in test]\n",
        "  dataset_splits[i][\"test_y\"] = normal_many([j[1] for j in test])[:,432:]\n",
        "  dataset_splits[i][\"train_smiles\"] = [j[0] for j in train]\n",
        "  dataset_splits[i][\"train_y\"] = normal_many([j[1] for j in train])[:,432:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN6YNridFKyx"
      },
      "source": [
        "Define Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrwfhVyP8s-i"
      },
      "outputs": [],
      "source": [
        "def euc_dist_keras(y_true, y_pred):\n",
        "    \"\"\"Euclidean distance loss function\"\"\"\n",
        "    return K.sqrt(K.sum(K.square(y_true - y_pred), axis=-1, keepdims=True))\n",
        "\n",
        "def pearson_first(y_true, y_pred):\n",
        "    \"\"\"Return pearson correlation for two single tensors\"\"\"\n",
        "    return(pearsonr(y_true, y_pred)[0])\n",
        "\n",
        "def wrapped_pearson_correlation(y_true, y_pred):\n",
        "    y = tf.py_function(func = pearson_first, inp = [y_true, y_pred], Tout = tf.float32)\n",
        "    return(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS4054Nmvl-N"
      },
      "source": [
        "Run SMILES through DC Featurizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REoPd3un1a3g"
      },
      "outputs": [],
      "source": [
        "featurizer = dc.feat.CircularFingerprint(radius = 2, size = 1024, chiral = False, features = False)\n",
        "for i in range(1, 6):\n",
        "  dataset_splits[i][\"test_x\"] = featurizer.featurize(dataset_splits[i][\"test_smiles\"])\n",
        "  dataset_splits[i][\"train_x\"] = featurizer.featurize(dataset_splits[i][\"train_smiles\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP5Oz1pzuriK"
      },
      "source": [
        "Add Graph Featurization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uhyi3MLFup8R"
      },
      "outputs": [],
      "source": [
        "graph_featurizer = dc.feat.ConvMolFeaturizer()\n",
        "for i in range(1, 6):\n",
        "  dataset_splits[i][\"test_x_graph\"] = graph_featurizer.featurize(dataset_splits[i][\"test_smiles\"])\n",
        "  dataset_splits[i][\"train_x_graph\"] = graph_featurizer.featurize(dataset_splits[i][\"train_smiles\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCpPQlVEvkh5"
      },
      "source": [
        "Graph Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JfuUHQyvj8d"
      },
      "outputs": [],
      "source": [
        "def data_generator(dataset, epochs = 1):\n",
        "    for ind, (X_b, y_b, w_b, ids_b) in enumerate(dataset.iterbatches(batch_size, epochs, deterministic = True, pad_batches = True)):\n",
        "        multiConvMol = ConvMol.agglomerate_mols(X_b)\n",
        "        inputs = [multiConvMol.get_atom_features(), multiConvMol.deg_slice, multiConvMol.membership]\n",
        "        for i in range(1, len(multiConvMol.get_deg_adjacency_lists())):\n",
        "            inputs.append(multiConvMol.get_deg_adjacency_lists()[i])\n",
        "            inputs = np.array(inputs)\n",
        "            labels = [y_b]\n",
        "            weights = [w_b]\n",
        "\n",
        "    yield (inputs, labels, weights)\n",
        "\n",
        "def clean(arr):\n",
        "    #Helper Function for DC featurizers\n",
        "    arr = list(map(float, arr))\n",
        "    return [item for item in arr if not np.isnan(item)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQq8KIl87FzZ"
      },
      "source": [
        "Other Models Featurizers and Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEeHjtRb7JrL"
      },
      "outputs": [],
      "source": [
        "#GraphConv featurization\n",
        "graph_featurizer = dc.feat.ConvMolFeaturizer()\n",
        "for i in range(1, 6):\n",
        "  dataset_splits[i][\"test_x_graph\"] = graph_featurizer.featurize(dataset_splits[i][\"test_smiles\"])\n",
        "  dataset_splits[i][\"train_x_graph\"] = graph_featurizer.featurize(dataset_splits[i][\"train_smiles\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YO6EuPF17NIS"
      },
      "outputs": [],
      "source": [
        "#Weave featurization\n",
        "mpnn = dc.feat.WeaveFeaturizer()\n",
        "for i in range(1, 6):\n",
        "  dataset_splits[i][\"test_x_mpnn\"] = mpnn.featurize(dataset_splits[i][\"test_smiles\"])\n",
        "  dataset_splits[i][\"train_x_mpnn\"] = mpnn.featurize(dataset_splits[i][\"train_smiles\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MolGraphConv featurization\n",
        "gat = dc.feat.MolGraphConvFeaturizer(use_edges=True)\n",
        "for i in range(1, 6):\n",
        "  dataset_splits[i][\"test_x_mgc\"] = list(gat.featurize(dataset_splits[i][\"test_smiles\"]))\n",
        "  dataset_splits[i][\"train_x_mgc\"] = list(gat.featurize(dataset_splits[i][\"train_smiles\"]))"
      ],
      "metadata": {
        "id": "lhKsDAO8_9xS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GCN"
      ],
      "metadata": {
        "id": "IAIAuze6XI7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#GraphConvModel training loop\n",
        "import pickle\n",
        "for i in range(1, 6):\n",
        "    dtrain = dc.data.NumpyDataset(X = dataset_splits[i][\"train_x_graph\"], y = dataset_splits[i][\"train_y\"])\n",
        "    dtest = dc.data.NumpyDataset(X = dataset_splits[i][\"test_x_graph\"], y = dataset_splits[i][\"test_y\"])\n",
        "    gcnmodel = dc.models.GraphConvModel(1800, mode='regression', dropout = 0.1, batch_normalize = True, dense_layer_size=2048, batch_size = 64, learning_rate = 0.001, activation_fns = [tf.nn.sigmoid, tf.nn.sigmoid, tf.nn.sigmoid, tf.nn.sigmoid, tf.nn.sigmoid, tf.nn.sigmoid, tf.nn.sigmoid])\n",
        "    gcnmodel.fit(dtrain, nb_epoch = 100)\n",
        "\n",
        "    #Collect evaluation metrics\n",
        "    g1predictions = gcnmodel.predict(dtest)\n",
        "    graph_r2s = []\n",
        "    total_r2, count = 0, 0\n",
        "    for x in range(len(dataset_splits[i][\"test_y\"])):\n",
        "        current_r2 = wrapped_pearson_correlation(normalize(g1predictions[x]), dataset_splits[i][\"test_y\"][x])\n",
        "        total_r2 += 0 if np.isnan(current_r2) else current_r2\n",
        "        graph_r2s.append(current_r2)\n",
        "        count += 1\n",
        "    current_fold_loss = round(float(total_r2 / count), 5)\n",
        "    print(\"R2 Loss for fold\", i, \":\", current_fold_loss)\n",
        "    gr2 = clean(list(map(float, graph_r2s)))\n",
        "    print(\"I:\", i, \"Mean\", statistics.mean(gr2), \"Median\", statistics.median(gr2), \"STDev\", statistics.stdev(gr2))#expirement\n",
        "    fold_predictions_path = path + \"GC_\" + str(i) + \"_preds.pickle\"\n",
        "    with open(fold_predictions_path, 'wb') as handle:\n",
        "        pickle.dump(g1predictions, handle)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6o5S9d1XHFX",
        "outputId": "11c2f283-cd25-447a-898f-7603c61e4378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 Loss for fold 1 : 0.78268\n",
            "I: 1 Mean 0.7826799059858786 Median 0.8166015148162842 STDev 0.16707830807219093\n",
            "R2 Loss for fold 2 : 0.75424\n",
            "I: 2 Mean 0.7542366366548413 Median 0.7876438498497009 STDev 0.17751610789087374\n",
            "R2 Loss for fold 3 : 0.81345\n",
            "I: 3 Mean 0.8134498006825646 Median 0.8551957309246063 STDev 0.15041162379188527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 Loss for fold 4 : 0.78765\n",
            "I: 4 Mean 0.7881760836351028 Median 0.8426811099052429 STDev 0.18534759265484863\n",
            "R2 Loss for fold 5 : 0.77158\n",
            "I: 5 Mean 0.7715848307598402 Median 0.8202866613864899 STDev 0.17708218476898757\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MPNN"
      ],
      "metadata": {
        "id": "lY9YslEL_xX_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qe9aLN6y9WJs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2480a1d-35b9-49a2-d1a0-1417c664400b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  assert input.numel() == input.storage().size(), (\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 Loss for fold 1 : 0.86846\n",
            "I: 1 Mean 0.8684604604648479 Median 0.9117663502693176 STDev 0.14041430869531368\n",
            "R2 Loss for fold 2 : 0.86286\n",
            "I: 2 Mean 0.8628573239156982 Median 0.9146058559417725 STDev 0.15230558812402234\n",
            "R2 Loss for fold 3 : 0.87391\n",
            "I: 3 Mean 0.8739131643250585 Median 0.9148865342140198 STDev 0.13244216886571603\n",
            "R2 Loss for fold 4 : 0.8592\n",
            "I: 4 Mean 0.8597730922071818 Median 0.9120941460132599 STDev 0.1576911603480587\n",
            "R2 Loss for fold 5 : 0.87027\n",
            "I: 5 Mean 0.8702684227367216 Median 0.9176366031169891 STDev 0.1448158321177\n"
          ]
        }
      ],
      "source": [
        "#MPNN model training loop\n",
        "from deepchem.models.torch_models import MPNNModel\n",
        "import dgl\n",
        "\n",
        "for i in range(1, 6):\n",
        "    dtrain = dc.data.NumpyDataset(X = dataset_splits[i][\"train_x_mgc\"], y = dataset_splits[i][\"train_y\"])\n",
        "    dtest = dc.data.NumpyDataset(X = dataset_splits[i][\"test_x_mgc\"], y = dataset_splits[i][\"test_y\"])\n",
        "    model = MPNNModel(1800, mode='regression', dropout = 0.1, batch_normalize = True, dense_layer_size=2048, batch_size = 64, n_pair_feat = 14, n_atom_feat = 75)\n",
        "    model.fit(dtrain, nb_epoch = 100)\n",
        "    #Collect evaluation metrics\n",
        "    graph_r2s = []\n",
        "    g2predictions = model.predict(dtest)\n",
        "    total_r2, count = 0, 0\n",
        "    for x in range(len(dataset_splits[i][\"test_y\"])):\n",
        "        current_r2 = wrapped_pearson_correlation(normalize(g2predictions[x]), dataset_splits[i][\"test_y\"][x])\n",
        "        total_r2 += 0 if np.isnan(current_r2) else current_r2\n",
        "        graph_r2s.append(current_r2)\n",
        "        count += 1\n",
        "    current_fold_loss = round(float(total_r2 / count), 5)\n",
        "    print(\"R2 Loss for fold\", i, \":\", current_fold_loss)\n",
        "    gr2 = clean(list(map(float, graph_r2s)))\n",
        "    print(\"I:\", i, \"Mean\", statistics.mean(gr2), \"Median\", statistics.median(gr2), \"STDev\", statistics.stdev(gr2))\n",
        "    fold_predictions_path = path + \"MPNN_\" + str(i) + \"_preds.pickle\"\n",
        "    with open(fold_predictions_path, 'wb') as handle:\n",
        "        pickle.dump(g2predictions, handle)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AttentiveFP"
      ],
      "metadata": {
        "id": "HVI2Y6f__u0b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUnOQjXJ9cjH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f12240c3-9658-4e92-e246-510c87080f21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 Loss for fold 1 : 0.88973\n",
            "I: 1 Mean 0.8897326930117688 Median 0.9348607063293457 STDev 0.13656071402891767\n",
            "R2 Loss for fold 2 : 0.88605\n",
            "I: 2 Mean 0.8860499247659496 Median 0.9350899457931519 STDev 0.13984195648038159\n",
            "R2 Loss for fold 3 : 0.88952\n",
            "I: 3 Mean 0.8895247695371509 Median 0.9374888837337494 STDev 0.13027806755725838\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 Loss for fold 4 : 0.88083\n",
            "I: 4 Mean 0.881416023201111 Median 0.9341515004634857 STDev 0.1495300033128919\n",
            "R2 Loss for fold 5 : 0.88883\n",
            "I: 5 Mean 0.8888323200757012 Median 0.9384139478206635 STDev 0.13985106077743867\n"
          ]
        }
      ],
      "source": [
        "#AttentiveFP model training loop\n",
        "\n",
        "\n",
        "import deepchem as dc\n",
        "from deepchem.models import AttentiveFPModel\n",
        "for i in range(1, 6):\n",
        "    dtrain = dc.data.NumpyDataset(X = dataset_splits[i][\"train_x_mgc\"], y = dataset_splits[i][\"train_y\"])\n",
        "    dtest = dc.data.NumpyDataset(X = dataset_splits[i][\"test_x_mgc\"], y = dataset_splits[i][\"test_y\"])\n",
        "    fpmodel = AttentiveFPModel(n_tasks = 1800, mode='regression', dropout = 0.1, batch_normalize = True, dense_layer_size=2048, batch_size = 64, learning_rate = 0.001, activation_fns = \"p\")\n",
        "\n",
        "    fpmodel.fit(dtrain, nb_epoch = 100)\n",
        "    fppredictions = fpmodel.predict(dtest)\n",
        "\n",
        "    graph_r2s = []\n",
        "    total_r2, count = 0, 0\n",
        "    for x in range(len(dataset_splits[i][\"test_y\"])):\n",
        "        current_r2 = wrapped_pearson_correlation(normalize(fppredictions[x]), dataset_splits[i][\"test_y\"][x])\n",
        "        total_r2 += 0 if np.isnan(current_r2) else current_r2\n",
        "        graph_r2s.append(current_r2)\n",
        "        count += 1\n",
        "    current_fold_loss = round(float(total_r2 / count), 5)\n",
        "    print(\"R2 Loss for fold\", i, \":\", current_fold_loss)\n",
        "    gr2 = clean(list(map(float, graph_r2s)))\n",
        "    print(\"I:\", i, \"Mean\", statistics.mean(gr2), \"Median\", statistics.median(gr2), \"STDev\", statistics.stdev(gr2))#expirement\n",
        "    fold_predictions_path = path + \"AFP_\" + str(i) + \"_preds.pickle\"\n",
        "    with open(fold_predictions_path, 'wb') as handle:\n",
        "        pickle.dump(fppredictions, handle)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GAT Model"
      ],
      "metadata": {
        "id": "lIOsF3_t_sBg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9xEjxDR9fH_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "806d43a1-e5ef-4b81-8351-acc7170afcbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 Loss for fold 1 : 0.81116\n",
            "I: 1 Mean 0.811163037599652 Median 0.8485574722290039 STDev 0.1533507744152288\n",
            "R2 Loss for fold 2 : 0.80345\n",
            "I: 2 Mean 0.8034500384062568 Median 0.8431214094161987 STDev 0.15703145370518626\n",
            "R2 Loss for fold 3 : 0.81251\n",
            "I: 3 Mean 0.812512859771649 Median 0.8478283584117889 STDev 0.14583377658594088\n",
            "R2 Loss for fold 4 : 0.80498\n",
            "I: 4 Mean 0.8055115403759818 Median 0.8454619348049164 STDev 0.16049066387768288\n",
            "R2 Loss for fold 5 : 0.80723\n",
            "I: 5 Mean 0.8072259483670935 Median 0.8390827775001526 STDev 0.14946917200372078\n"
          ]
        }
      ],
      "source": [
        "#GAT model training loop\n",
        "for i in range(1, 6):\n",
        "    dtrain = dc.data.NumpyDataset(X = dataset_splits[i][\"train_x_mgc\"], y = dataset_splits[i][\"train_y\"])\n",
        "    dtest = dc.data.NumpyDataset(X = dataset_splits[i][\"test_x_mgc\"], y = dataset_splits[i][\"test_y\"])\n",
        "    model = dc.models.GATModel(1800, mode='regression', dropout = 0.1, graph_attention_layers = [64, 64], batch_normalize = True, dense_layer_size=2048, batch_size = 64, learning_rate = 0.001)\n",
        "    model.fit(dtrain, nb_epoch = 100)\n",
        "    gatpredictions = model.predict(dtest)\n",
        "\n",
        "    graph_r2s = []\n",
        "    total_r2, count = 0, 0\n",
        "    for x in range(len(dataset_splits[i][\"test_y\"])):\n",
        "        current_r2 = wrapped_pearson_correlation(normalize(gatpredictions[x]), dataset_splits[i][\"test_y\"][x])\n",
        "        total_r2 += 0 if np.isnan(current_r2) else current_r2\n",
        "        graph_r2s.append(current_r2)\n",
        "        count += 1\n",
        "    current_fold_loss = round(float(total_r2 / count), 5)\n",
        "    print(\"R2 Loss for fold\", i, \":\", current_fold_loss)\n",
        "    gr2 = clean(list(map(float, graph_r2s)))\n",
        "    print(\"I:\", i, \"Mean\", statistics.mean(gr2), \"Median\", statistics.median(gr2), \"STDev\", statistics.stdev(gr2))#expirement\n",
        "\n",
        "    fold_predictions_path = path + \"GAT_\" + str(i) + \"_preds.pickle\"\n",
        "    with open(fold_predictions_path, 'wb') as handle:\n",
        "        pickle.dump(gatpredictions, handle)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MorganFP/DNN Model"
      ],
      "metadata": {
        "id": "aecHV3H6_pgl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iId76mTgKd3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "325d96ec-38bd-4871-c3cc-4e9e4d39c2bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47/47 [==============================] - 0s 2ms/step\n",
            "R2 Loss for fold 1 : 0.86806\n",
            "I 1 0.8680567330681408 0.9198525547981262 0.15146768710067954\n",
            "47/47 [==============================] - 0s 2ms/step\n",
            "R2 Loss for fold 2 : 0.86172\n",
            "I 2 0.8617206382405952 0.9236180186271667 0.15817543832473255\n",
            "47/47 [==============================] - 0s 2ms/step\n",
            "R2 Loss for fold 3 : 0.87118\n",
            "I 3 0.8711828431952745 0.9273976683616638 0.1468683184148113\n",
            "48/48 [==============================] - 0s 2ms/step\n",
            "R2 Loss for fold 4 : 0.85997\n",
            "I 4 0.8605389280338831 0.9221876263618469 0.16141339017634002\n",
            "47/47 [==============================] - 0s 3ms/step\n",
            "R2 Loss for fold 5 : 0.86896\n",
            "I 5 0.8689623830795169 0.9259579479694366 0.15243854976736723\n"
          ]
        }
      ],
      "source": [
        "#MorganFP model using a dense layer as output\n",
        "for i in range(1, 6):\n",
        "    fpmodel = Sequential()\n",
        "    fpmodel.add(Dense(4096, input_dim = 1024))\n",
        "    fpmodel.add(BatchNormalization())\n",
        "    fpmodel.add(Dropout(0.1))\n",
        "    fpmodel.add(Dense(2048, activation = \"relu\"))\n",
        "    fpmodel.add(BatchNormalization())\n",
        "    fpmodel.add(Dropout(0.1))\n",
        "    fpmodel.add(Dense(1024, activation = \"relu\"))\n",
        "\n",
        "    fpmodel.add(Dense(1800, activation = \"sigmoid\"))\n",
        "\n",
        "    fpmodel.compile(loss = euc_dist_keras, optimizer = \"Adam\")\n",
        "    fpmodel.fit(dataset_splits[i][\"train_x\"], dataset_splits[i][\"train_y\"], batch_size = 64, epochs = 100, verbose = 0)\n",
        "    #Collect evaluation metrics\n",
        "    morganpredictions = fpmodel.predict(dataset_splits[i][\"test_x\"])\n",
        "    total_r2, count = 0, 0\n",
        "    totalp = 0\n",
        "    fp_r2s = []\n",
        "    for x in range(len(morganpredictions)):\n",
        "        current_r2 = wrapped_pearson_correlation(normalize(morganpredictions[x]), dataset_splits[i][\"test_y\"][x])\n",
        "        total_r2 += 0 if np.isnan(current_r2) else current_r2\n",
        "        fp_r2s.append(current_r2)\n",
        "        count += 1\n",
        "\n",
        "    current_fold_loss = round(float(total_r2 / count), 5)\n",
        "    print(\"R2 Loss for fold\", i, \":\", current_fold_loss)\n",
        "    cleanfpr2s = clean(list(map(float, fp_r2s)))\n",
        "    print(\"I\", i, statistics.mean(cleanfpr2s), statistics.median(cleanfpr2s), statistics.stdev(cleanfpr2s))\n",
        "    fold_predictions_path = path + \"MFP_\" + str(i) + \"_preds.pickle\"\n",
        "    with open(fold_predictions_path, 'wb') as handle:\n",
        "        pickle.dump(morganpredictions, handle)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}